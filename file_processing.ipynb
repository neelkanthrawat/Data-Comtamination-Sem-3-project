{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neel/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strg + shift + 7 for multi lines comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory (where the notebook is running)\n",
    "script_dir = os.getcwd()\n",
    "\n",
    "# Construct the path to your folder (assuming it's inside the repo)\n",
    "dataset_folder_path = os.path.join(script_dir, \"datasets\")\n",
    "\n",
    "# # Read the file\n",
    "# with open(file_path, \"r\") as f:\n",
    "#     content = f.read()\n",
    "#     print(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a file inside that folder\n",
    "input_file = os.path.join(dataset_folder_path, \"winogrande_validation.jsonl\")\n",
    "output_file = os.path.join(dataset_folder_path, \"winogrande_val_splitup.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Load the dataset (assuming JSONL format)\n",
    "def load_winogrande(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "# Save the processed dataset\n",
    "def save_dataset(data, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "# Function to split a sentence, prioritizing \".\" (not at end), \"than\", then other punctuation\n",
    "def split_sentence(sentence):\n",
    "    # Prioritize \".\" but only if it's not at the end\n",
    "    match = re.search(r\"\\.(?!$)\", sentence)  # Ensures the period is not at the end\n",
    "    if match:\n",
    "        idx = match.start() + 1\n",
    "        return sentence[:idx], sentence[idx:].strip()\n",
    "\n",
    "    # If no \".\", prioritize other punctuation (; , :)\n",
    "    match = re.search(r\"([,;:])\", sentence)\n",
    "    if match:\n",
    "        idx = match.start() + 1  # Keep punctuation in part1\n",
    "        return sentence[:idx], sentence[idx:].strip()\n",
    "\n",
    "    # If \"than\" exists, split after the compared entity\n",
    "    idx = sentence.find(\" than \")\n",
    "    if idx != -1:\n",
    "        next_space = sentence.find(\" \", idx + 6)  # Look after \" than \"\n",
    "        if next_space != -1:\n",
    "            return sentence[:next_space], sentence[next_space:].strip()\n",
    "\n",
    "    # If no punctuation or \"than\", split in half at a word boundary\n",
    "    mid = len(sentence) // 2\n",
    "    while mid > 0 and sentence[mid] != \" \":\n",
    "        mid -= 1\n",
    "    if mid == 0:\n",
    "        mid = len(sentence) // 2\n",
    "        while mid < len(sentence) and sentence[mid] != \" \":\n",
    "            mid += 1\n",
    "    \n",
    "    return sentence[:mid].strip(), sentence[mid:].strip()\n",
    "\n",
    "# Process dataset\n",
    "def process_dataset(dataset):\n",
    "    new_data = []\n",
    "    for item in dataset:\n",
    "        sentence = item[\"sentence\"]\n",
    "        part1, part2 = split_sentence(sentence)\n",
    "        new_data.append({\"part1\": part1, \"part2\": part2, **item})  # Keep original fields\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset saved to /home/neel/Desktop/Data-Comtamination-Sem-3-project/datasets/winogrande_val_splitup.jsonl\n"
     ]
    }
   ],
   "source": [
    "dataset = load_winogrande(input_file)\n",
    "processed_data = process_dataset(dataset)  # Randomly choose method\n",
    "save_dataset(processed_data, output_file)\n",
    "\n",
    "print(f\"Processed dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for CB dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_file = os.path.join(dataset_folder_path, \"CommitmentBank-items.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file (update 'your_file.csv' with the actual file path)\n",
    "df = pd.read_csv(cb_file)\n",
    "\n",
    "# Extract relevant columns\n",
    "df_context = df[['uID', 'Context']]\n",
    "df_embedding = df[['uID', 'Embedding']]\n",
    "df_target = df[['uID', 'Target']]\n",
    "\n",
    "# Save to separate CSV files\n",
    "df_context.to_csv(\"context_sentences.csv\", index=False)\n",
    "df_embedding.to_csv(\"prompt_sentences.csv\", index=False)\n",
    "df_target.to_csv(\"target_sentences.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(cb_file)[['uID', 'Embedding','Context', 'Target']].to_csv(\"cb_sentences.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
